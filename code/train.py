'''
Author: GitHub@Tom-Liii
The code is adapted from CSDN@HMTT

ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºCSDNåšä¸»ã€ŒHMTTã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚
åŸæ–‡é“¾æ¥ï¼šhttps://blog.csdn.net/qq_42464569/article/details/123898549
'''
from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments, BertConfig
import torch
from datasets import Dataset
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import json
import os
# è®¾å®šä½¿ç”¨çš„GPUç¼–å·ï¼Œä¹Ÿå¯ä»¥ä¸è®¾ç½®ï¼Œä½†trainerä¼šé»˜è®¤ä½¿ç”¨å¤šGPU
# os.environ["CUDA_VISIBLE_DEVICES"] = "1"
# ä½¿ç”¨cpuè¿›è¡Œè®­ç»ƒ

os.environ["WANDB_DISABLED"] = "true"

def compute_metrics(pred):
    labels = pred.label_ids.argmax(-1)
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')
    acc = accuracy_score(labels, preds)
    print('---Evaluation Completed---')
    print('Accuracy = %.2f%%'%(acc*100))
    print('Precision = %.2f%%'%(precision*100))
    print('f1 = %.2f'%(f1))
    return {
        'accuracy': acc,
        # 'f1': f1,
        # 'precision': precision,
        # 'recall': recall
    }


# å°†num_labelsè®¾ç½®ä¸º2ï¼Œå› ä¸ºæˆ‘ä»¬è®­ç»ƒçš„ä»»åŠ¡ä¸º2åˆ†ç±»
# model = BertForSequenceClassification.from_pretrained('../transformersModels/bert-base-uncased/', num_labels=2)
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

train_dataset = Dataset.load_from_disk('./data_new/train_dataset/')
test_dataset = Dataset.load_from_disk('./data_new/test_dataset/')

# for param in model.base_model.parameters():
#     param.requires_grad = False

# è®­ç»ƒè¶…å‚é…ç½®
training_args = TrainingArguments(
    output_dir='./my_results',          # output directory ç»“æœè¾“å‡ºåœ°å€
    num_train_epochs=10,              # total # of training epochs è®­ç»ƒæ€»æ‰¹æ¬¡
    per_device_train_batch_size=4,  # batch size per device during training è®­ç»ƒæ‰¹å¤§å°
    per_device_eval_batch_size=128,   # batch size for evaluation è¯„ä¼°æ‰¹å¤§å°
    logging_dir='./my_logs',            # directory for storing logs æ—¥å¿—å­˜å‚¨ä½ç½®
    learning_rate=1e-6,			# learning rate
)



# åˆ›å»ºTrainer
trainer = Trainer(
    model=model.cuda(),              # the instantiated ğŸ¤— Transformers model to be trained éœ€è¦è®­ç»ƒçš„æ¨¡å‹
    args=training_args,                  # training arguments, defined above è®­ç»ƒå‚æ•°
    train_dataset=train_dataset,         # training dataset è®­ç»ƒé›†
    eval_dataset=test_dataset,           # evaluation dataset æµ‹è¯•é›†
    compute_metrics=compute_metrics
)


# å¼€å§‹è®­ç»ƒ
trainer.train()

# å¼€å§‹è¯„ä¼°æ¨¡å‹
trainer.evaluate()

# ä¿å­˜æ¨¡å‹ ä¼šä¿å­˜åˆ°é…ç½®çš„output_dirå¤„
trainer.save_model()

'''
# æ¨¡å‹é…ç½®æ–‡ä»¶
config.json

# æ¨¡å‹æ•°æ®æ–‡ä»¶
pytorch-model.bin

# è®­ç»ƒé…ç½®æ–‡ä»¶
training_args.bin
'''

# load the saved model
output_config_file = './my_results/config.json'
output_model_file = './my_results/pytorch_model.bin'

config = BertConfig.from_json_file(output_config_file)
model = BertForSequenceClassification(config)
state_dict = torch.load(output_model_file)
model.load_state_dict(state_dict)

# use nlp to test the model
# cache_dir="../transformersModels/bert-base-uncased/"
cache_dir="bert-base-uncased"
tokenizer = BertTokenizer.from_pretrained(cache_dir)
data = tokenizer(['This is a good movie', 'This is a bad movie'], max_length=512, truncation=True, padding='max_length', return_tensors="pt")
print(model(**data))

